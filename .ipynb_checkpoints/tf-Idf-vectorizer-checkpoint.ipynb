{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8fa7215f84466a9d2f7fc75bb0a75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042dff7e69934eb3a1ac10511d6922f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9a4ab94f5546bd932a06d9d54d2f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e496410efcd42eb8848cffbe8b3cec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c83b968823a4cadad21e0b76da8e01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667f4e45799a482f8cc50b1e4d561027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d019c8fddc43ff870562122aa126ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ea5050763749b28ff451f042410842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6282dc47ff8c4a678adbd08b8949efe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d955ec3479d4526aedd8223ee836f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfa321d9f5849aab98933a0f1315b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cab7937e7f4c959f89ba3d10a81454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7fc4b0db8043c680aca3004ddc4fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e4eba9253042bc9bc1370e1af884a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20cb946be98477a8e5877007bc12aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac21bcf95a7f498d93798ff4fe0c4ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# read datasets from csv files, first column is review, second column is rating/sentiment \n",
    "imdb_training_data = pd.read_csv('IMDB-train.txt', sep = \"\\t\", header = None)\n",
    "imdb_validation_data = pd.read_csv('IMDB-valid.txt', sep = \"\\t\", header = None)\n",
    "imdb_test_data = pd.read_csv('IMDB-test.txt', sep = \"\\t\", header = None)\n",
    "\n",
    "yelp_training_data = pd.read_csv('yelp-train.txt', sep = \"\\t\", header = None)\n",
    "yelp_validation_data = pd.read_csv('yelp-valid.txt', sep = \"\\t\", header = None)\n",
    "yelp_test_data = pd.read_csv('yelp-test.txt', sep = \"\\t\", header = None)\n",
    "    \n",
    "imdb_train_reviews, imdb_train_labels = [review.replace('<br /><br />', ' ') for review in imdb_training_data[0]],list(imdb_training_data[1])\n",
    "imdb_valid_reviews, imdb_valid_labels = [review.replace('<br /><br />', ' ') for review in imdb_validation_data[0]], list(imdb_validation_data[1])\n",
    "imdb_test_reviews, imdb_test_labels = [review.replace('<br /><br />', ' ') for review in imdb_test_data[0]], list(imdb_test_data[1])\n",
    "\n",
    "yelp_train_reviews, yelp_train_labels = [review.replace('<br /><br />', ' ') for review in yelp_training_data[0]],list(yelp_training_data[1])\n",
    "yelp_valid_reviews, yelp_valid_labels = [review.replace('<br /><br />', ' ') for review in yelp_validation_data[0]], list(yelp_validation_data[1])\n",
    "yelp_test_reviews, yelp_test_labels = [review.replace('<br /><br />', ' ') for review in yelp_test_data[0]], list(yelp_test_data[1])\n",
    "\n",
    "# When lemmatizing, we need to convert from NLTK's part of speec\n",
    "# to wordnet's recognized parts of speech\n",
    "def get_wordnet_pos(treebank_pos):\n",
    "    if treebank_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def sentence_tokenize(sentence, lem = WordNetLemmatizer()):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return [lem.lemmatize(w, pos=get_wordnet_pos(pos)) for (w, pos) in tagged_tokens]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    input = \"content\",\n",
    "    tokenizer = sentence_tokenize,\n",
    "    max_features = 10000\n",
    ")\n",
    "\n",
    "tuple_tfidf_vectorizer = TfidfVectorizer(\n",
    "    input = \"content\",\n",
    "    tokenizer = sentence_tokenize,\n",
    "    ngram_range = (1, 2),\n",
    "    max_features = 10000\n",
    ")\n",
    "\n",
    "def tfidf_vectorize(sentences, ngram=False):\n",
    "    if ngram:\n",
    "        return tuple_tfidf_vectorizer.transform(tqdm_notebook(sentences))\n",
    "    else:\n",
    "        return tfidf_vectorizer.transform(tqdm_notebook(sentences))\n",
    "\n",
    "\n",
    "# Fit vectorizer on imdb training and validation \n",
    "\n",
    "tfidf_vectorizer.fit(tqdm_notebook(imdb_train_reviews + imdb_valid_reviews))\n",
    "tuple_tfidf_vectorizer.fit(tqdm_notebook(imdb_train_reviews + imdb_valid_reviews))\n",
    "\n",
    "# Vectorize for imdb dataset\n",
    "vectorized_train_data_imdb_bow = tfidf_vectorize(imdb_train_reviews)\n",
    "vectorized_valid_data_imdb_bow = tfidf_vectorize(imdb_valid_reviews)\n",
    "vectorized_test_data_imdb_bow = tfidf_vectorize(imdb_test_reviews)\n",
    "vectorized_train_data_imdb_bow = vectorized_train_data_imdb_bow.toarray()\n",
    "vectorized_valid_data_imdb_bow = vectorized_valid_data_imdb_bow.toarray()\n",
    "vectorized_test_data_imdb_bow = vectorized_test_data_imdb_bow.toarray()\n",
    "\n",
    "vectorized_train_data_imdb_bigram = tfidf_vectorize(imdb_train_reviews,ngram=True)\n",
    "vectorized_valid_data_imdb_bigram = tfidf_vectorize(imdb_valid_reviews,ngram=True)\n",
    "vectorized_test_data_imdb_bigram = tfidf_vectorize(imdb_test_reviews,ngram=True)\n",
    "vectorized_train_data_imdb_bigram = vectorized_train_data_imdb_bigram.toarray()\n",
    "vectorized_valid_data_imdb_bigram = vectorized_valid_data_imdb_bigram.toarray()\n",
    "vectorized_test_data_imdb_bigram = vectorized_test_data_imdb_bigram.toarray()\n",
    "\n",
    "# Vectorize yelp dataset\n",
    "tfidf_vectorizer.fit(tqdm_notebook(yelp_train_reviews + yelp_valid_reviews))\n",
    "tuple_tfidf_vectorizer.fit(tqdm_notebook(yelp_train_reviews + yelp_valid_reviews))\n",
    "\n",
    "vectorized_train_data_yelp_bow = tfidf_vectorize(yelp_train_reviews)\n",
    "vectorized_valid_data_yelp_bow = tfidf_vectorize(yelp_valid_reviews)\n",
    "vectorized_test_data_yelp_bow = tfidf_vectorize(yelp_test_reviews)\n",
    "vectorized_train_data_yelp_bow = vectorized_train_data_yelp_bow.toarray()\n",
    "vectorized_valid_data_yelp_bow = vectorized_valid_data_yelp_bow.toarray()\n",
    "vectorized_test_data_yelp_bow = vectorized_test_data_yelp_bow.toarray()\n",
    "\n",
    "vectorized_train_data_yelp_bigram = tfidf_vectorize(yelp_train_reviews,ngram=True)\n",
    "vectorized_valid_data_yelp_bigram = tfidf_vectorize(yelp_valid_reviews,ngram=True)\n",
    "vectorized_test_data_yelp_bigram = tfidf_vectorize(yelp_test_reviews,ngram=True)\n",
    "vectorized_train_data_yelp_bigram = vectorized_train_data_yelp_bigram.toarray()\n",
    "vectorized_valid_data_yelp_bigram = vectorized_valid_data_yelp_bigram.toarray()\n",
    "vectorized_test_data_yelp_bigram = vectorized_test_data_yelp_bigram.toarray()\n",
    "\n",
    "# Predefine split for training and validation data, for use when cross validating\n",
    "ps_imdb = PredefinedSplit([-1 for s in imdb_train_reviews] + [0 for s in imdb_valid_reviews])\n",
    "ps_yelp = PredefinedSplit([-1 for s in yelp_train_reviews] + [0 for s in yelp_valid_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Bag of words Multinomial Naive Bayes (IMDB): {'alpha': 0.1}\n",
      "Optimal accuracy of Bag of Words Multinomial Naive Bayes on IMDB dataset: 0.82936\n",
      "Best params for Bigram Multinomial Naive Bayes (IMDB): {'alpha': 0.0001}\n",
      "Optimal accuracy of Bigram Multinomial Naive Bayes on IMDB dataset: 0.85812\n",
      "Best params for Bag of words Multinomial Naive Bayes (Yelp): {'alpha': 0.1}\n",
      "Optimal accuracy of Bag of Words Multinomial Naive Bayes on Yelp dataset: 0.464\n",
      "Best params for Bigram Multinomial Naive Bayes (Yelp): {'alpha': 0.1}\n",
      "Optimal accuracy of Bigram Multinomial Naive Bayes on Yelp dataset: 0.523\n"
     ]
    }
   ],
   "source": [
    "# Hypertune for Multinomial Naive Bayes on imdb\n",
    "parameters = {\"alpha\": [1e-4, 0.01, 0.1, 1.0, 2.0, 10.0]}\n",
    "\n",
    "# Bag of words IMDB\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bow, vectorized_valid_data_imdb_bow)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bag of words Multinomial Naive Bayes (IMDB):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Multinomial Naive Bayes on IMDB dataset:', grid.score(vectorized_test_data_imdb_bow, imdb_test_labels))\n",
    "\n",
    "# Bigram IMDB\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bigram, vectorized_valid_data_imdb_bigram)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bigram Multinomial Naive Bayes (IMDB):\", grid.best_params_)\n",
    "\n",
    "print('Optimal accuracy of Bigram Multinomial Naive Bayes on IMDB dataset:', grid.score(vectorized_test_data_imdb_bigram, imdb_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bow, vectorized_valid_data_yelp_bow)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bag of words Multinomial Naive Bayes (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Multinomial Naive Bayes on Yelp dataset:', grid.score(vectorized_test_data_yelp_bow, yelp_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bigram, vectorized_valid_data_yelp_bigram)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bigram Multinomial Naive Bayes (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bigram Multinomial Naive Bayes on Yelp dataset:', grid.score(vectorized_test_data_yelp_bigram, yelp_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Bag of words Linear SVM (IMDB): {'C': 1.1119999999999999, 'tol': 1e-09}\n",
      "Optimal accuracy of Bag of Words Linear SVM on IMDB dataset: 0.873\n",
      "Best params for Bigram Linear SVM (IMDB): {'C': 1.1119999999999999, 'tol': 1e-09}\n",
      "Optimal accuracy of Bigram Linear SVM on IMDB dataset: 0.88756\n",
      "Best params for Bag of words Linear SVM (Yelp): {'C': 1.1119999999999999, 'tol': 1e-09}\n",
      "Optimal accuracy of Bag of Words Linear SVM on Yelp dataset: 0.4915\n",
      "Best params for Bigram Linear SVM  (Yelp): {'C': 1.1119999999999999, 'tol': 1e-09}\n",
      "Optimal accuracy of Bigram Linear SVM on Yelp dataset: 0.512\n"
     ]
    }
   ],
   "source": [
    "#Hypertune for Linear SVM on imdb\n",
    "parameters = {'C':np.linspace(0.001, 10, 10), 'tol':np.linspace(1e-9, 1e-5, 5)}\n",
    "\n",
    "# Bag of words IMDB\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bow, vectorized_valid_data_imdb_bow)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bag of words Linear SVM (IMDB):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Linear SVM on IMDB dataset:', grid.score(vectorized_test_data_imdb_bow, imdb_test_labels))\n",
    "\n",
    "# Bigram IMDB\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bigram, vectorized_valid_data_imdb_bigram)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bigram Linear SVM (IMDB):\", grid.best_params_)\n",
    "\n",
    "print('Optimal accuracy of Bigram Linear SVM on IMDB dataset:', grid.score(vectorized_test_data_imdb_bigram, imdb_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bow, vectorized_valid_data_yelp_bow)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bag of words Linear SVM (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Linear SVM on Yelp dataset:', grid.score(vectorized_test_data_yelp_bow, yelp_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bigram, vectorized_valid_data_yelp_bigram)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bigram Linear SVM  (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bigram Linear SVM on Yelp dataset:', grid.score(vectorized_test_data_yelp_bigram, yelp_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Bag of words Decision Trees Classifier (IMDB): {'criterion': 'gini', 'max_depth': 22, 'splitter': 'random'}\n",
      "Optimal accuracy of Bag of Words Decision Trees Classifier on IMDB dataset: 0.73932\n",
      "Best params for Bigram Decision Trees Classifier (IMDB): {'criterion': 'gini', 'max_depth': 19, 'splitter': 'random'}\n",
      "Optimal accuracy of Bigram Decision Trees Classifier on IMDB dataset: 0.7284\n",
      "Best params for Bag of words Decision Trees Classifier (Yelp): {'criterion': 'gini', 'max_depth': 16, 'splitter': 'random'}\n",
      "Optimal accuracy of Bag of Words Decision Trees Classifier on Yelp dataset: 0.359\n",
      "Best params for Bigram Decision Trees Classifier (Yelp): {'criterion': 'gini', 'max_depth': 19, 'splitter': 'random'}\n",
      "Optimal accuracy of Bigram Decision Trees Classifier on Yelp dataset: 0.3985\n"
     ]
    }
   ],
   "source": [
    "# Hypertune for decision trees on imdb\n",
    "parameters = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random'],'max_depth':range(15, 26, 1)}\n",
    "\n",
    "# Bag of words IMDB\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bow, vectorized_valid_data_imdb_bow)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bag of words Decision Trees Classifier (IMDB):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Decision Trees Classifier on IMDB dataset:', grid.score(vectorized_test_data_imdb_bow, imdb_test_labels))\n",
    "\n",
    "# Bigram IMDB\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bigram, vectorized_valid_data_imdb_bigram)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bigram Decision Trees Classifier (IMDB):\", grid.best_params_)\n",
    "\n",
    "print('Optimal accuracy of Bigram Decision Trees Classifier on IMDB dataset:', grid.score(vectorized_test_data_imdb_bigram, imdb_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bow, vectorized_valid_data_yelp_bow)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bag of words Decision Trees Classifier (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Decision Trees Classifier on Yelp dataset:', grid.score(vectorized_test_data_yelp_bow, yelp_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bigram, vectorized_valid_data_yelp_bigram)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bigram Decision Trees Classifier (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bigram Decision Trees Classifier on Yelp dataset:', grid.score(vectorized_test_data_yelp_bigram, yelp_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
