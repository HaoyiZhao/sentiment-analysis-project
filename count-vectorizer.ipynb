{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/howie/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dacf611bb16478ab4bc0f28ec452153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5e2743d6434669981aa0df7653501b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ad1710b37643719c6337d0e0c2a7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969a515eee5a427190c576d5f5baadfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84a9feda740421eb690438e3638d823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d79591936d458bb4acdc68d0c33eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da760a60972c4b1fa3f66497ce905fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12be0e30fcc4482b920a823c5e917a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f3e02631ff4598a82c64860f4079ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341ae09da2d24f919879820d224c0f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aad0ae87d647fcb83ba8c3885e5afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b2b7aa04dd4726a346ab4d7782efcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82289754efd144acb3d5545c3dc11ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-066a37bf674f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mvectorized_test_data_yelp_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorized_test_data_yelp_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mvectorized_train_data_yelp_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0mvectorized_valid_data_yelp_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_valid_reviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mvectorized_test_data_yelp_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_test_reviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# read datasets from csv files, first column is review, second column is rating/sentiment \n",
    "imdb_training_data = pd.read_csv('IMDB-train.txt', sep = \"\\t\", header = None)\n",
    "imdb_validation_data = pd.read_csv('IMDB-valid.txt', sep = \"\\t\", header = None)\n",
    "imdb_test_data = pd.read_csv('IMDB-test.txt', sep = \"\\t\", header = None)\n",
    "\n",
    "yelp_training_data = pd.read_csv('yelp-train.txt', sep = \"\\t\", header = None)\n",
    "yelp_validation_data = pd.read_csv('yelp-valid.txt', sep = \"\\t\", header = None)\n",
    "yelp_test_data = pd.read_csv('yelp-test.txt', sep = \"\\t\", header = None)\n",
    "    \n",
    "imdb_train_reviews, imdb_train_labels = [review.replace('<br /><br />', ' ') for review in imdb_training_data[0]],list(imdb_training_data[1])\n",
    "imdb_valid_reviews, imdb_valid_labels = [review.replace('<br /><br />', ' ') for review in imdb_validation_data[0]], list(imdb_validation_data[1])\n",
    "imdb_test_reviews, imdb_test_labels = [review.replace('<br /><br />', ' ') for review in imdb_test_data[0]], list(imdb_test_data[1])\n",
    "\n",
    "yelp_train_reviews, yelp_train_labels = [review.replace('<br /><br />', ' ') for review in yelp_training_data[0]],list(yelp_training_data[1])\n",
    "yelp_valid_reviews, yelp_valid_labels = [review.replace('<br /><br />', ' ') for review in yelp_validation_data[0]], list(yelp_validation_data[1])\n",
    "yelp_test_reviews, yelp_test_labels = [review.replace('<br /><br />', ' ') for review in yelp_test_data[0]], list(yelp_test_data[1])\n",
    "\n",
    "# When lemmatizing, we need to convert from NLTK's part of speec\n",
    "# to wordnet's recognized parts of speech\n",
    "def get_wordnet_pos(treebank_pos):\n",
    "    if treebank_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def sentence_tokenize(sentence, lem = WordNetLemmatizer()):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return [lem.lemmatize(w, pos=get_wordnet_pos(pos)) for (w, pos) in tagged_tokens]\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    input = \"content\",\n",
    "    tokenizer = sentence_tokenize,\n",
    "    max_features = 10000\n",
    ")\n",
    "\n",
    "tuple_count_vectorizer = CountVectorizer(\n",
    "    input = \"content\",\n",
    "    tokenizer = sentence_tokenize,\n",
    "    ngram_range = (1, 2),\n",
    "    max_features = 10000\n",
    ")\n",
    "\n",
    "def count_vectorize(sentences, ngram=False):\n",
    "    if ngram:\n",
    "        return tuple_count_vectorizer.transform(tqdm_notebook(sentences))\n",
    "    else:\n",
    "        return count_vectorizer.transform(tqdm_notebook(sentences))\n",
    "\n",
    "\n",
    "# Fit vectorizer on imdb training and validation \n",
    "\n",
    "count_vectorizer.fit(tqdm_notebook(imdb_train_reviews + imdb_valid_reviews))\n",
    "tuple_count_vectorizer.fit(tqdm_notebook(imdb_train_reviews + imdb_valid_reviews))\n",
    "\n",
    "# Vectorize for imdb dataset\n",
    "vectorized_train_data_imdb_bow = count_vectorize(imdb_train_reviews)\n",
    "vectorized_valid_data_imdb_bow = count_vectorize(imdb_valid_reviews)\n",
    "vectorized_test_data_imdb_bow = count_vectorize(imdb_test_reviews)\n",
    "vectorized_train_data_imdb_bow = vectorized_train_data_imdb_bow.toarray()\n",
    "vectorized_valid_data_imdb_bow = vectorized_valid_data_imdb_bow.toarray()\n",
    "vectorized_test_data_imdb_bow = vectorized_test_data_imdb_bow.toarray()\n",
    "\n",
    "vectorized_train_data_imdb_bigram = count_vectorize(imdb_train_reviews,ngram=True)\n",
    "vectorized_valid_data_imdb_bigram = count_vectorize(imdb_valid_reviews,ngram=True)\n",
    "vectorized_test_data_imdb_bigram = count_vectorize(imdb_test_reviews,ngram=True)\n",
    "vectorized_train_data_imdb_bigram = vectorized_train_data_imdb_bigram.toarray()\n",
    "vectorized_valid_data_imdb_bigram = vectorized_valid_data_imdb_bigram.toarray()\n",
    "vectorized_test_data_imdb_bigram = vectorized_test_data_imdb_bigram.toarray()\n",
    "\n",
    "# Vectorize yelp dataset\n",
    "count_vectorizer.fit(tqdm_notebook(yelp_train_reviews + yelp_valid_reviews))\n",
    "tuple_count_vectorizer.fit(tqdm_notebook(yelp_train_reviews + yelp_valid_reviews))\n",
    "\n",
    "vectorized_train_data_yelp_bow = count_vectorize(yelp_train_reviews)\n",
    "vectorized_valid_data_yelp_bow = count_vectorize(yelp_valid_reviews)\n",
    "vectorized_test_data_yelp_bow = count_vectorize(yelp_test_reviews)\n",
    "vectorized_train_data_yelp_bow = vectorized_train_data_yelp_bow.toarray()\n",
    "vectorized_valid_data_yelp_bow = vectorized_valid_data_yelp_bow.toarray()\n",
    "vectorized_test_data_yelp_bow = vectorized_test_data_yelp_bow.toarray()\n",
    "\n",
    "vectorized_train_data_yelp_bigram = count_vectorize(yelp_train_reviews,ngram=True)\n",
    "vectorized_valid_data_yelp_bigram = count_vectorize(yelp_valid_reviews,ngram=True)\n",
    "vectorized_test_data_yelp_bigram = count_vectorize(yelp_test_reviews,ngram=True)\n",
    "vectorized_train_data_yelp_bigram = vectorized_train_data_yelp_bigram.toarray()\n",
    "vectorized_valid_data_yelp_bigram = vectorized_valid_data_yelp_bigram.toarray()\n",
    "vectorized_test_data_yelp_bigram = vectorized_test_data_yelp_bigram.toarray()\n",
    "\n",
    "# Predefine split for training and validation data, for use when cross validating\n",
    "ps_imdb = PredefinedSplit([-1 for s in imdb_train_reviews] + [0 for s in imdb_valid_reviews])\n",
    "ps_yelp = PredefinedSplit([-1 for s in yelp_train_reviews] + [0 for s in yelp_valid_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Multinomial Naive Bayes: {'alpha': 0.01}\n",
      "Optimal accuracy of Multinomial Naive Bayes on IMDB dataset: 0.82088\n"
     ]
    }
   ],
   "source": [
    "# Hypertune for Multinomial Naive Bayes on imdb\n",
    "parameters = {\"alpha\": [1e-4, 0.01, 0.1, 1.0, 2.0, 10.0]}\n",
    "\n",
    "# Bag of words IMDB\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bow, vectorized_valid_data_imdb_bow)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bag of words Multinomial Naive Bayes (IMDB):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Multinomial Naive Bayes on IMDB dataset:', grid.score(vectorized_test_data_imdb_bow, imdb_test_labels))\n",
    "\n",
    "# Bigram IMDB\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bigram, vectorized_valid_data_imdb_bigram)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bigram Multinomial Naive Bayes (IMDB):\", grid.best_params_)\n",
    "\n",
    "print('Optimal accuracy of Bigram Multinomial Naive Bayes on IMDB dataset:', grid.score(vectorized_test_data_imdb_bigram, imdb_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bow, vectorized_valid_yelp_yelp_bow)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bag of words Multinomial Naive Bayes (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Multinomial Naive Bayes on Yelp dataset:', grid.score(vectorized_test_data_yelp_bow, yelp_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = MultinomialNB()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bigram, vectorized_valid_data_yelp_bigram)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bigram Multinomial Naive Bayes (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bigram Multinomial Naive Bayes on Yelp dataset:', grid.score(vectorized_test_data_yelp_bigram, yelp_test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Linear SVM: {'C': 0.001, 'max_iter': 1000, 'tol': 1e-09}\n",
      "Optimal accuracy of Linear SVC on IMDB dataset: 0.88104\n"
     ]
    }
   ],
   "source": [
    "#Hypertune for Linear SVM on imdb\n",
    "parameters = {'C':np.linspace(0.001, 10, 10), 'tol':np.linspace(1e-9, 1e-5, 5), \"max_iter\": range(1000, 10001, 1000)}\n",
    "\n",
    "# Bag of words IMDB\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bow, vectorized_valid_data_imdb_bow)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bag of words Linear SVM (IMDB):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Linear SVM on IMDB dataset:', grid.score(vectorized_test_data_imdb_bow, imdb_test_labels))\n",
    "\n",
    "# Bigram IMDB\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bigram, vectorized_valid_data_imdb_bigram)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bigram Linear SVM (IMDB):\", grid.best_params_)\n",
    "\n",
    "print('Optimal accuracy of Bigram Linear SVM on IMDB dataset:', grid.score(vectorized_test_data_imdb_bigram, imdb_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bow, vectorized_valid_yelp_yelp_bow)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bag of words Linear SVM (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Linear SVM on Yelp dataset:', grid.score(vectorized_test_data_yelp_bow, yelp_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = LinearSVC()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bigram, vectorized_valid_data_yelp_bigram)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bigram Linear SVM  (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bigram Linear SVM on Yelp dataset:', grid.score(vectorized_test_data_yelp_bigram, yelp_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Decision Trees Classifier: {'criterion': 'gini', 'max_depth': 20, 'splitter': 'random'}\n",
      "Optimal accuracy of Decision Trees Classifier on IMDB dataset: 0.73352\n"
     ]
    }
   ],
   "source": [
    "# Hypertune for decision trees on imdb\n",
    "parameters = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random'],'max_depth':range(15, 26, 1)}\n",
    "\n",
    "# Bag of words IMDB\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bow, vectorized_valid_data_imdb_bow)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bag of words Decision Trees Classifier (IMDB):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Decision Trees Classifier on IMDB dataset:', grid.score(vectorized_test_data_imdb_bow, imdb_test_labels))\n",
    "\n",
    "# Bigram IMDB\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bigram, vectorized_valid_data_imdb_bigram)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bigram Decision Trees Classifier (IMDB):\", grid.best_params_)\n",
    "\n",
    "print('Optimal accuracy of Bigram Decision Trees Classifier on IMDB dataset:', grid.score(vectorized_test_data_imdb_bigram, imdb_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bow, vectorized_valid_yelp_yelp_bow)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bag of words Decision Trees Classifier (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Decision Trees Classifier on Yelp dataset:', grid.score(vectorized_test_data_yelp_bow, yelp_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bigram, vectorized_valid_data_yelp_bigram)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bigram Decision Trees Classifier (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bigram Decision Trees Classifier on Yelp dataset:', grid.score(vectorized_test_data_yelp_bigram, yelp_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Random Forests Classifier: {'criterion': 'entropy', 'max_features': 'auto', 'n_estimators': 100}\n",
      "Optimal accuracy of Random Forests Classifier on IMDB dataset: 0.8476\n"
     ]
    }
   ],
   "source": [
    "# Hypertune for Random Forests on imdb\n",
    "parameters = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[10,20,50,100],\n",
    "          'max_features':['auto', 'log2', 100, 200]}\n",
    "\n",
    "# Bag of words IMDB\n",
    "clf = RandomForestClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bow, vectorized_valid_data_imdb_bow)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bag of words Random Forests Classifier (IMDB):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Random Forests Classifier on IMDB dataset:', grid.score(vectorized_test_data_imdb_bow, imdb_test_labels))\n",
    "\n",
    "# Bigram IMDB\n",
    "clf = RandomForestClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_imdb)\n",
    "grid.fit(np.concatenate((vectorized_train_data_imdb_bigram, vectorized_valid_data_imdb_bigram)), imdb_train_labels + imdb_valid_labels)\n",
    "print(\"Best params for Bigram Random Forests Classifier (IMDB):\", grid.best_params_)\n",
    "\n",
    "print('Optimal accuracy of Bigram Random Forests Classifier on IMDB dataset:', grid.score(vectorized_test_data_imdb_bigram, imdb_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = RandomForestClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bow, vectorized_valid_yelp_yelp_bow)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bag of words Random Forests Classifier (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bag of Words Random Forests Classifier on Yelp dataset:', grid.score(vectorized_test_data_yelp_bow, yelp_test_labels))\n",
    "\n",
    "# Bag of words Yelp\n",
    "clf = RandomForestClassifier()\n",
    "grid = GridSearchCV(clf, parameters, cv=ps_yelp)\n",
    "grid.fit(np.concatenate((vectorized_train_data_yelp_bigram, vectorized_valid_data_yelp_bigram)), yelp_train_labels + yelp_valid_labels)\n",
    "print(\"Best params for Bigram Random Forests Classifier (Yelp):\", grid.best_params_)\n",
    "print('Optimal accuracy of Bigram Random Forests Classifier on Yelp dataset:', grid.score(vectorized_test_data_yelp_bigram, yelp_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
