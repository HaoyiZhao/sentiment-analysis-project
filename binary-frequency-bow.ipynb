{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert datasets into binary and frequency bag of words representations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# read datasets from csv files, first column is review, second column is rating/sentiment \n",
    "imdb_training_data = pd.read_csv('IMDB-train.txt', sep = \"\\t\", header = None)\n",
    "imdb_validation_data = pd.read_csv('IMDB-valid.txt', sep = \"\\t\", header = None)\n",
    "imdb_test_data = pd.read_csv('IMDB-test.txt', sep = \"\\t\", header = None)\n",
    "\n",
    "yelp_training_data = pd.read_csv('yelp-train.txt', sep = \"\\t\", header = None)\n",
    "yelp_validation_data = pd.read_csv('yelp-valid.txt', sep = \"\\t\", header = None)\n",
    "yelp_test_data = pd.read_csv('yelp-test.txt', sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert datasets into binary and frequency bag of words representations\n",
    "\n",
    "# Method takes as input a review Data Frame and returns the top 10,000 words with highest frequency in the form of a tuple:\n",
    "# (top words dictionary with word as key and rank as value, top words information in format specified by pdf\n",
    "# that will be output into file)\n",
    "def find_top_words(reviews_data):\n",
    "    words = []\n",
    "    top_words = []\n",
    "    \n",
    "    for review in reviews_data[0]:\n",
    "        # preprocess by removing punctuation and <br /><br />, and converting reviews to lower case\n",
    "        words.extend(review.lower().replace('<br /><br />', ' ').translate(str.maketrans(\"\",\"\", string.punctuation)).split(' '))\n",
    "        \n",
    "    # remove empty strings from words list\n",
    "    words = list(filter(None, words))\n",
    "    \n",
    "    # Use Counter to find most 10,000 common words from word list, returned as tuple (string word, integer count)\n",
    "    top_words = Counter(words).most_common(10000)\n",
    "    \n",
    "    # First element of tuple is dictionary of most common word String as key, frequency rank (starting at index 0) as value\n",
    "    # Second element of tuple is List of Strings (1 for each top word) which consists of word, rank, and frequency delimited by tabs\n",
    "    # enumerate enumerates top_words list, so that we can assign unique ID/rank to dictionary, and output string\n",
    "    return {top_word[0]: index for index, top_word in enumerate(top_words)}, [top_word[0] + '\\t' + str(index) + '\\t' + str(top_word[1])  for index, top_word in enumerate(top_words)]\n",
    "    \n",
    "# find top words and output from training data sets\n",
    "imdb_top_words, imdb_output = find_top_words(imdb_training_data)\n",
    "yelp_top_words, yelp_output = find_top_words(yelp_training_data)\n",
    "    \n",
    "# method takes as input tuple list top_words (String word, frequency) and review data and returns array of vector representations of reviews, with a \n",
    "# 1 in the index of vector if the word at that index in top_words appears in the specific review\n",
    "def generate_binary_bag_of_words_representation(top_words, reviews_data):\n",
    "    vectors = []\n",
    "    \n",
    "    for review in reviews_data[0]:\n",
    "        # preprocess review and split into individual words\n",
    "        review_words = review.lower().replace('<br /><br />', ' ').translate(str.maketrans(\"\",\"\", string.punctuation)).split(' ')\n",
    "        # initialize vector to contain 10,000 0's\n",
    "        current_vector = [0] * 10000\n",
    "        # for each word in review, if word is in list of top words, set current_vector at index corresponding to id of top word to 1\n",
    "        for word in review_words:\n",
    "            if word in top_words:\n",
    "                current_vector[top_words[word]] = 1\n",
    "        vectors.append(current_vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "# method takes as input tuple list top_words (String word, frequency) and review data and returns array of vector representations of reviews, based on\n",
    "# frequency representation. Values of each vector will sum to 1, and each non-zero value in vector will correspond to its proportional\n",
    "# occurrence weight in the specific review (feature[id] = (#id in review)/#(all ids in review)\n",
    "def generate_frequency_bag_of_words_representation(top_words, reviews_data):\n",
    "    vectors = []\n",
    "    \n",
    "    for review in reviews_data[0]:\n",
    "        # preprocess review and split into individual words\n",
    "        review_words = review.lower().replace('<br /><br />', ' ').translate(str.maketrans(\"\",\"\", string.punctuation)).split(' ')\n",
    "        # initialize vector to contain 10,000 0's\n",
    "        current_vector = [0] * 10000\n",
    "        # for each word in review, if word is in list of top words, set current_vector at index corresponding to id of top word to 1\n",
    "        for word in review_words:\n",
    "            # if word is in top_words, increment vector at index given by word id from dictionary\n",
    "            if word in top_words:\n",
    "                current_vector[top_words[word]] += 1\n",
    "        \n",
    "        # calculate sum of all top_word frequencies in current review\n",
    "        top_words_sum = sum(current_vector)\n",
    "        \n",
    "        # if there is at least 1 top word in current review, divide all frequencies in vector by total number of top word frequencies in vector\n",
    "        # to yield proportion of given top words then add to vectors list, otherwise just add zero vector to vectors list\n",
    "        if top_words_sum > 0:\n",
    "            current_vector = np.divide(current_vector, top_words_sum)\n",
    "        vectors.append(current_vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "# convert datasets to binary and frequency bag of words representations\n",
    "imdb_training_binary_bow_data = generate_binary_bag_of_words_representation(imdb_top_words, imdb_training_data)\n",
    "imdb_validation_binary_bow_data = generate_binary_bag_of_words_representation(imdb_top_words, imdb_validation_data)\n",
    "imdb_test_binary_bow_data = generate_binary_bag_of_words_representation(imdb_top_words, imdb_test_data)\n",
    "imdb_training_frequency_bow_data = generate_frequency_bag_of_words_representation(imdb_top_words, imdb_training_data)\n",
    "imdb_validation_frequency_bow_data = generate_frequency_bag_of_words_representation(imdb_top_words, imdb_validation_data)\n",
    "imdb_test_frequency_bow_data = generate_frequency_bag_of_words_representation(imdb_top_words, imdb_test_data)\n",
    "\n",
    "yelp_training_binary_bow_data = generate_binary_bag_of_words_representation(yelp_top_words, yelp_training_data)\n",
    "yelp_validation_binary_bow_data = generate_binary_bag_of_words_representation(yelp_top_words, yelp_validation_data)\n",
    "yelp_test_binary_bow_data = generate_binary_bag_of_words_representation(yelp_top_words, yelp_test_data)\n",
    "yelp_training_frequency_bow_data = generate_frequency_bag_of_words_representation(yelp_top_words, yelp_training_data)\n",
    "yelp_validation_frequency_bow_data = generate_frequency_bag_of_words_representation(yelp_top_words, yelp_validation_data)\n",
    "yelp_test_frequency_bow_data = generate_frequency_bag_of_words_representation(yelp_top_words, yelp_test_data)\n",
    "\n",
    "ps_imdb_bbow = PredefinedSplit([-1 for s in imdb_training_binary_bow_data] + [0 for s in imdb_validation_binary_bow_data])\n",
    "ps_imdb_fbow = PredefinedSplit([-1 for s in imdb_training_frequency_bow_data] + [0 for s in imdb_validation_frequency_bow_data])\n",
    "ps_yelp_bbow = PredefinedSplit([-1 for s in yelp_training_binary_bow_data] + [0 for s in yelp_validation_binary_bow_data])\n",
    "ps_yelp_fbow = PredefinedSplit([-1 for s in yelp_training_frequency_bow_data] + [0 for s in yelp_validation_frequency_bow_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random classifier performance on yelp training data:  0.19942857142857146\n",
      "Random classifier performance on yelp validation data:  0.199\n",
      "Random classifier performance on yelp test data:  0.17900000000000002\n",
      "Majority classifier performance on yelp training data:  0.3525714285714286\n",
      "Majority classifier performance on yelp validation data:  0.356\n",
      "Majority classifier performance on yelp test data:  0.351\n"
     ]
    }
   ],
   "source": [
    "# Test yelp data set with binary bag of words representation\n",
    "\n",
    "# method calculates and returns the random classifier f1_score performance given the input data classifications\n",
    "# and range of classification values which will be used to generate random classifications\n",
    "def report_random_classifier_performance(data, classification_range):\n",
    "    random_classifications = np.random.choice(classification_range, len(data))\n",
    "    return sklearn.metrics.f1_score(data, random_classifications, average = 'micro')\n",
    "    \n",
    "# method takes as input classification data and returns majority classifier performance in the form of a f1_score\n",
    "def report_majority_class_classifier_performance(data):\n",
    "    # np.bincount returns array of counts for each index value seen in input data\n",
    "    # np.argmax returns the index of the highest count, which results in the majority class\n",
    "    majority_class = np.argmax(np.bincount(data))\n",
    "    majority_classifications = [majority_class] * len(data)\n",
    "    return sklearn.metrics.f1_score(data, majority_classifications, average = 'micro')\n",
    "    \n",
    "def main(): \n",
    "    print('Random classifier performance on yelp training data: ',report_random_classifier_performance(yelp_training_data[1], range(1,6)))\n",
    "    print('Random classifier performance on yelp validation data: ',report_random_classifier_performance(yelp_validation_data[1], range(1,6)))\n",
    "    print('Random classifier performance on yelp test data: ',report_random_classifier_performance(yelp_test_data[1], range(1,6)))\n",
    "    print('Majority classifier performance on yelp training data: ',report_majority_class_classifier_performance(yelp_training_data[1]))\n",
    "    print('Majority classifier performance on yelp validation data: ',report_majority_class_classifier_performance(yelp_validation_data[1]))\n",
    "    print('Majority classifier performance on yelp test data: ',report_majority_class_classifier_performance(yelp_test_data[1]))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Bernoulli Naive Bayes: {'alpha': 0.01}\n",
      "Optimal accuracy of Bernoulli Naive Bayes on Yelp dataset (bbow): 0.4415\n",
      "Best params for Multinomial Naive Bayes: {'criterion': 'entropy', 'max_depth': 10.0, 'splitter': 'best'}\n",
      "Optimal accuracy of Decision Tree Classifier on Yelp dataset (bbow): 0.3965\n",
      "Best params for Multinomial Naive Bayes: {'C': 0.001, 'max_iter': 1000, 'tol': 1e-09}\n",
      "Optimal accuracy of Linear SVM Classifier on Yelp dataset (bbow): 0.504\n"
     ]
    }
   ],
   "source": [
    "# method tunes for optimal hyperparameters for the Bernoulli Naive Bayes classifier using the yelp training and validation data,\n",
    "# then classifies the test data using the trained classifier with the optimal hyperparameters\n",
    "def hypertune_BNB_hyperparameters_yelp_bbow():\n",
    "    parameters = {\"alpha\": [1e-4, 0.01, 0.1, 1.0, 2.0, 10.0]}\n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "    grid_bnb = GridSearchCV(clf, parameters, cv=ps_yelp_bbow)\n",
    "    grid_bnb.fit(np.concatenate((yelp_training_binary_bow_data, yelp_validation_binary_bow_data)), list(yelp_training_data[1]) + list(yelp_validation_data[1]))\n",
    "    print(\"Best params for Bernoulli Naive Bayes:\", grid_bnb.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Bernoulli Naive Bayes on Yelp dataset (bbow):', grid_bnb.score(yelp_test_binary_bow_data, yelp_test_data[1]))\n",
    "    \n",
    "    \n",
    "# method tunes for optimal hyperparameters for the Decision Trees classifier using the yelp training and validation data,\n",
    "# then classifies the test data using the trained classifier with the optimal hyperparameters\n",
    "def hypertune_decision_tree_hyperparameters_yelp_bbow():\n",
    "    parameters = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random'],  'max_depth':np.linspace(6,15,10)}\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    grid_tree = GridSearchCV(clf, parameters, cv=ps_yelp_bbow)\n",
    "    grid_tree.fit(np.concatenate((yelp_training_binary_bow_data, yelp_validation_binary_bow_data)), list(yelp_training_data[1]) + list(yelp_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_tree.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Decision Tree Classifier on Yelp dataset (bbow):', grid_tree.score(yelp_test_binary_bow_data, yelp_test_data[1]))\n",
    "\n",
    "# method tunes for optimal hyperparameters for the linear Support Vector Machine classifier using the yelp training and validation data,\n",
    "# then classifies the test data using the trained classifier with the optimal hyperparameters\n",
    "def hypertune_SVM_hyperparameters_yelp_bbow():\n",
    "    parameters={'C':np.linspace(0.001, 10, 5), 'tol':np.linspace(1e-9, 1e-5, 5), \"max_iter\": range(1000, 10001, 1000)}\n",
    "    \n",
    "    clf = LinearSVC()\n",
    "    grid_svc = GridSearchCV(clf, parameters, cv=ps_yelp_bbow)\n",
    "    grid_svc.fit(np.concatenate((yelp_training_binary_bow_data, yelp_validation_binary_bow_data)), list(yelp_training_data[1]) + list(yelp_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_svc.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Linear SVM Classifier on Yelp dataset (bbow):', grid_svc.score(yelp_test_binary_bow_data, yelp_test_data[1]))\n",
    "    \n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\")  \n",
    "    hypertune_BNB_hyperparameters_yelp_bbow()\n",
    "    hypertune_decision_tree_hyperparameters_yelp_bbow()\n",
    "    hypertune_SVM_hyperparameters_yelp_bbow()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Gaussian Naive Bayes: {'var_smoothing': 1.0816326530612246}\n",
      "Optimal accuracy of Gaussian Naive Bayes on Yelp dataset (fbow): 0.3635\n",
      "Best params for Multinomial Naive Bayes: {'criterion': 'gini', 'max_depth': 8.0, 'splitter': 'best'}\n",
      "Optimal accuracy of Decision Tree Classifier on Yelp dataset (fbow): 0.391\n",
      "Best params for Multinomial Naive Bayes: {'C': 7.50025, 'max_iter': 1000, 'tol': 1e-09}\n",
      "Optimal accuracy of Linear SVM Classifier on Yelp dataset (fbow): 0.5075\n"
     ]
    }
   ],
   "source": [
    "#Test Yelp data with Frequency bag of words representation\n",
    "\n",
    "def hypertune_GNB_hyperparameters_yelp_fbow():\n",
    "    parameters = {\"var_smoothing\":np.linspace(1, 5, 50)}\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    grid_gnb = GridSearchCV(clf, parameters, cv=ps_yelp_fbow)\n",
    "    grid_gnb.fit(np.concatenate((yelp_training_frequency_bow_data, yelp_validation_frequency_bow_data)), list(yelp_training_data[1]) + list(yelp_validation_data[1]))\n",
    "    print(\"Best params for Gaussian Naive Bayes:\", grid_gnb.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Gaussian Naive Bayes on Yelp dataset (fbow):', grid_gnb.score(yelp_test_frequency_bow_data, yelp_test_data[1]))\n",
    "  \n",
    "def hypertune_decision_tree_hyperparameters_yelp_fbow():\n",
    "    parameters = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random'],  'max_depth':np.linspace(6,15,10)}\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    grid_tree = GridSearchCV(clf, parameters, cv=ps_yelp_fbow)\n",
    "    grid_tree.fit(np.concatenate((yelp_training_frequency_bow_data, yelp_validation_frequency_bow_data)), list(yelp_training_data[1]) + list(yelp_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_tree.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Decision Tree Classifier on Yelp dataset (fbow):', grid_tree.score(yelp_test_frequency_bow_data, yelp_test_data[1]))\n",
    "    \n",
    "def hypertune_SVM_hyperparameters_yelp_fbow():\n",
    "    parameters={'C':np.linspace(0.001, 10, 5), 'tol':np.linspace(1e-9, 1e-5, 5), \"max_iter\": range(1000, 10001, 1000)}\n",
    "    \n",
    "    clf = LinearSVC()\n",
    "    grid_svc = GridSearchCV(clf, parameters, cv=ps_yelp_fbow)\n",
    "    grid_svc.fit(np.concatenate((yelp_training_frequency_bow_data, yelp_validation_frequency_bow_data)), list(yelp_training_data[1]) + list(yelp_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_svc.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Linear SVM Classifier on Yelp dataset (fbow):', grid_svc.score(yelp_test_frequency_bow_data, yelp_test_data[1]))\n",
    "    \n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    hypertune_GNB_hyperparameters_yelp_fbow()\n",
    "    hypertune_decision_tree_hyperparameters_yelp_fbow()\n",
    "    hypertune_SVM_hyperparameters_yelp_fbow()\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random classifier performance on IMDB training data:  0.4984\n",
      "Random classifier performance on IMDB validation data:  0.4969\n",
      "Random classifier performance on IMDB test data:  0.496\n",
      "Majority classifier performance on IMDB training data:  0.5\n",
      "Majority classifier performance on IMDB validation data:  0.5\n",
      "Majority classifier performance on IMDB test data:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Test IMDB data set with binary bag of words representation\n",
    "# Random classifier\n",
    "\n",
    "# method calculates and returns the random classifier f1_score performance given the input data classifications\n",
    "# and range of classification values which will be used to generate random classifications\n",
    "def report_random_classifier_performance(data, classification_range):\n",
    "    random_classifications = np.random.choice(classification_range, len(data))\n",
    "    return sklearn.metrics.f1_score(data, random_classifications, average = 'micro')\n",
    "    \n",
    "# method takes as input classification data and returns majority classifier performance in the form of a f1_score\n",
    "def report_majority_class_classifier_performance(data):\n",
    "    # np.bincount returns array of counts for each index value seen in input data\n",
    "    # np.argmax returns the index of the highest count, which results in the majority class\n",
    "    majority_class = np.argmax(np.bincount(data))\n",
    "    majority_classifications = [majority_class] * len(data)\n",
    "    return sklearn.metrics.f1_score(data, majority_classifications, average = 'micro')\n",
    "    \n",
    "def main(): \n",
    "    print('Random classifier performance on IMDB training data: ',report_random_classifier_performance(imdb_training_data[1], range(0,2)))\n",
    "    print('Random classifier performance on IMDB validation data: ',report_random_classifier_performance(imdb_validation_data[1], range(0,2)))\n",
    "    print('Random classifier performance on IMDB test data: ',report_random_classifier_performance(imdb_test_data[1], range(0,2)))\n",
    "    print('Majority classifier performance on IMDB training data: ',report_majority_class_classifier_performance(imdb_training_data[1]))\n",
    "    print('Majority classifier performance on IMDB validation data: ',report_majority_class_classifier_performance(imdb_validation_data[1]))\n",
    "    print('Majority classifier performance on IMDB test data: ',report_majority_class_classifier_performance(imdb_test_data[1]))\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Bernoulli Naive Bayes: {'alpha': 0.28354545454545454}\n",
      "Optimal accuracy of Bernoulli Naive Bayes on IMDB dataset (bbow): 0.84104\n",
      "Best params for Multinomial Naive Bayes: {'criterion': 'gini', 'max_depth': 15.0, 'splitter': 'best'}\n",
      "Optimal accuracy of Decision Tree Classifier on IMDB dataset (bbow): 0.72968\n",
      "Best params for Multinomial Naive Bayes: {'C': 0.001, 'max_iter': 1000, 'tol': 1e-09}\n",
      "Optimal accuracy of Linear SVM Classifier on IMDB dataset (bbow): 0.87976\n"
     ]
    }
   ],
   "source": [
    "def hypertune_BNB_hyperparameters_imdb_bbow():\n",
    "    parameters = {\"alpha\":np.linspace(0.001, 1, 100)}\n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "    grid_bnb = GridSearchCV(clf, parameters, cv=ps_imdb_bbow)\n",
    "    grid_bnb.fit(np.concatenate((imdb_training_binary_bow_data, imdb_validation_binary_bow_data)), list(imdb_training_data[1]) + list(imdb_validation_data[1]))\n",
    "    print(\"Best params for Bernoulli Naive Bayes:\", grid_bnb.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Bernoulli Naive Bayes on IMDB dataset (bbow):', grid_bnb.score(imdb_test_binary_bow_data, imdb_test_data[1]))\n",
    "    \n",
    "def hypertune_decision_tree_hyperparameters_imdb_bbow():\n",
    "    parameters = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random'],  'max_depth':np.linspace(6,15,10)}\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    grid_tree = GridSearchCV(clf, parameters, cv=ps_imdb_bbow)\n",
    "    grid_tree.fit(np.concatenate((imdb_training_binary_bow_data, imdb_validation_binary_bow_data)), list(imdb_training_data[1]) + list(imdb_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_tree.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Decision Tree Classifier on IMDB dataset (bbow):', grid_tree.score(imdb_test_binary_bow_data, imdb_test_data[1]))\n",
    "\n",
    "def hypertune_SVM_hyperparameters_imdb_bbow():\n",
    "    parameters={'C':np.linspace(0.001, 10, 5), 'tol':np.linspace(1e-9, 1e-5, 5), \"max_iter\": range(1000, 10001, 1000)}\n",
    "    \n",
    "    clf = LinearSVC()\n",
    "    grid_svc = GridSearchCV(clf, parameters, cv=ps_imdb_bbow)\n",
    "    grid_svc.fit(np.concatenate((imdb_training_binary_bow_data, imdb_validation_binary_bow_data)), list(imdb_training_data[1]) + list(imdb_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_svc.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Linear SVM Classifier on IMDB dataset (bbow):', grid_svc.score(imdb_test_binary_bow_data, imdb_test_data[1]))\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    hypertune_BNB_hyperparameters_imdb_bbow()\n",
    "    hypertune_decision_tree_hyperparameters_imdb_bbow()\n",
    "    hypertune_SVM_hyperparameters_imdb_bbow()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'range'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0aa182a6d96f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mhypertune_SVM_hyperparameters_imdb_fbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-0aa182a6d96f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mhypertune_GNB_hyperparameters_imdb_fbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mhypertune_decision_tree_hyperparameters_imdb_fbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mhypertune_SVM_hyperparameters_imdb_fbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-0aa182a6d96f>\u001b[0m in \u001b[0;36mhypertune_GNB_hyperparameters_imdb_fbow\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhypertune_GNB_hyperparameters_imdb_fbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"var_smoothing\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'range'"
     ]
    }
   ],
   "source": [
    "#Test imdb data with Frequency bag of words representation\n",
    "\n",
    "def hypertune_GNB_hyperparameters_imdb_fbow():\n",
    "    parameters = {\"var_smoothing\":range(0.001, 5, 100)}\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    grid_gnb = GridSearchCV(clf, parameters, cv=ps_imdb_fbow)\n",
    "    grid_gnb.fit(np.concatenate((imdb_training_frequency_bow_data, imdb_validation_frequency_bow_data)), list(imdb_training_data[1]) + list(imdb_validation_data[1]))\n",
    "    print(\"Best params for Gaussian Naive Bayes:\", grid_gnb.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Gaussian Naive Bayes on IMDB dataset (fbow):', grid_gnb.score(imdb_test_frequency_bow_data, imdb_test_data[1]))\n",
    "  \n",
    "def hypertune_decision_tree_hyperparameters_imdb_fbow():\n",
    "    parameters = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random'],  'max_depth':np.linspace(6,15,10)}\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "    grid_tree = GridSearchCV(clf, parameters, cv=ps_imdb_fbow)\n",
    "    grid_tree.fit(np.concatenate((imdb_training_frequency_bow_data, imdb_validation_frequency_bow_data)), list(imdb_training_data[1]) + list(imdb_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_tree.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Decision Tree Classifier on IMDB dataset (fbow):', grid_tree.score(imdb_test_frequency_bow_data, imdb_test_data[1]))\n",
    "    \n",
    "def hypertune_SVM_hyperparameters_imdb_fbow():\n",
    "    parameters={'C':np.linspace(0.001, 10, 5), 'tol':np.linspace(1e-9, 1e-5, 5), \"max_iter\": range(1000, 10001, 1000)}\n",
    "    \n",
    "    clf = LinearSVC()\n",
    "    grid_svc = GridSearchCV(clf, parameters, cv=ps_imdb_fbow)\n",
    "    grid_svc.fit(np.concatenate((imdb_training_frequency_bow_data, imdb_validation_frequency_bow_data)), list(imdb_training_data[1]) + list(imdb_validation_data[1]))\n",
    "    print(\"Best params for Multinomial Naive Bayes:\", grid_svc.best_params_)\n",
    "\n",
    "    print('Optimal accuracy of Linear SVM Classifier on IMDB dataset (fbow):', grid_svc.score(imdb_test_frequency_bow_data, imdb_test_data[1]))\n",
    "    \n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    hypertune_GNB_hyperparameters_imdb_fbow()\n",
    "    hypertune_decision_tree_hyperparameters_imdb_fbow()\n",
    "    hypertune_SVM_hyperparameters_imdb_fbow()\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
